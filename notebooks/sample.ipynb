{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ollama, langfuseのサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "PUBLIC_KEY = os.getenv(\"PUBLIC_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=PUBLIC_KEY,\n",
    "    secret_key=SECRET_KEY,\n",
    "    host=LANGFUSE_HOST,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s break down what LangChain is, step by step.\\n\\n**1. The Problem LangChain Solves: Building with LLMs is Hard**\\n\\n* **Large Language Models (LLMs) are Powerful:** Models like GPT-3, GPT-4, PaLM, Llama 2, and others can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\\n* **But LLMs Aren\\'t Standalone Solutions:** While impressive, they are often not directly usable for complex tasks. They excel at predicting the next word, but connecting that to a larger workflow, dealing with data, or remembering past interactions is tricky. They are *really* good at being prompt-driven, but you need to build the prompts and the surrounding infrastructure.\\n* **Building on Top of LLMs is Complex:** To build useful applications with LLMs, you need to handle things like:\\n    * **Data Connection:** How do you feed your data (documents, databases, APIs) to the LLM?\\n    * **Prompt Engineering:** Crafting the right instructions (prompts) to get the desired output from the LLM. This is more than just a single sentence; it often requires complex formatting and chaining together instructions.\\n    * **Chaining & Agents:** Combining multiple LLM calls and other components into a sequence to achieve a bigger goal.  How do you make one LLM call\\'s output the input for another? How do you handle decision-making?\\n    * **Memory:** LLMs themselves have limited memory. How do you remember past interactions and use them to inform future ones?\\n    * **Evaluation:** How do you measure the quality of the LLM\\'s output and improve your application?\\n\\n**2. LangChain\\'s Core Idea: A Framework for Building LLM-Powered Applications**\\n\\n* **It\\'s a Framework, Not a Model:**  LangChain *doesn\\'t* replace the LLMs themselves. It\\'s built *on top* of them.  Think of it as a toolkit or a library.\\n* **Abstraction & Modularity:** LangChain provides abstractions and modular components to handle all the complexities mentioned above. It breaks down the different aspects of building LLM applications into reusable pieces.\\n* **Key Components:** Here\\'s a simplified breakdown of some major areas LangChain addresses:\\n    * **Models:** Wrappers around different LLMs (OpenAI, Cohere, Hugging Face, etc.) to standardize interaction.\\n    * **Prompts:** Tools for creating, managing, and optimizing prompts.  This includes prompt templates for consistency and reusability.\\n    * **Chains:**  Sequences of calls – combining LLMs with other utilities. This is where the real power lies in creating multi-step workflows.  There are many pre-built chains for common tasks.\\n    * **Indexes (Data Connection):**  Structures for indexing and retrieving data that can be used as context for the LLM. Think of this as connecting the LLM to your own knowledge base. This includes document loaders, text splitters, and vectorstores.\\n    * **Memory:**  Mechanisms for storing and retrieving conversation history or other state information.  This allows LLMs to \"remember\" previous interactions.\\n    * **Agents:**  Systems that use LLMs to decide which actions to take. They can use tools to interact with the outside world (e.g., search the web, run code).\\n\\n**3.  Analogy: Think of a Lego Set**\\n\\n* **LLM:**  A single Lego brick – powerful on its own, but limited.\\n* **LangChain:**  A Lego set with instructions and different kinds of bricks. It gives you the pieces and guidelines to build something more complex and interesting.\\n* **Your Application:** The finished Lego model – a fully functional LLM-powered application.\\n\\n**4.  In Short:**\\n\\nLangChain is a framework designed to simplify the development of applications powered by large language models. It provides tools and abstractions to connect LLMs to data, manage prompts, create chains of operations, and build intelligent agents.\\n\\n\\n\\nDo you want me to elaborate on any specific aspect of LangChain, like a particular component or use case?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"gemma3:12b\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"}, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

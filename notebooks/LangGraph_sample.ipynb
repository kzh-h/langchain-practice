{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5478071c",
   "metadata": {},
   "source": [
    "# LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a98923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "from typing import Annotated, Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebef71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "PUBLIC_KEY = os.getenv(\"PUBLIC_KEY\")\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=PUBLIC_KEY,\n",
    "    secret_key=SECRET_KEY,\n",
    "    host=LANGFUSE_HOST,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a361af",
   "metadata": {},
   "source": [
    "## Ollamaを使ったLanggraphのサンプルコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb26ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    \"\"\"ステートクラス.\n",
    "\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(..., description=\"ユーザーからの質問\")\n",
    "    current_role: str = Field(default=\"\", description=\"選定された解答ロール\")\n",
    "    # operator.addは2つの値を加算するための関数\n",
    "    # ステート更新時にaddオペレーションにより、リストに要素が追加される。リストの足し算と同義\n",
    "    messages: Annotated[list[str], operator.add] = Field(default=[], description=\"解答履歴\")\n",
    "    current_judge: bool = Field(default=False, description=\"品質チェックの結果\")\n",
    "    judgment_reason: str = Field(default=\"\", description=\"品質チェックの判定理由\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3c2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLES = {\n",
    "    \"1\": {\n",
    "        \"name\": \"一般知識のエキスパート\",\n",
    "        \"description\": \"幅広い分野の一般的な質問に答える\",\n",
    "        \"details\": \"幅広い分野の一般的な質問に対して、正確でわかりやすい回答を提供してください。\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"name\": \"生成AI製品エキスパート\",\n",
    "        \"description\": \"生成AIや関連製品、技術に関する専門的な質問に答える\",\n",
    "        \"details\": \"生成AIや関連製品、技術に関する専門的な質問に対して、最新の情報と深い洞察を提供してください。\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"name\": \"カウンセラー\",\n",
    "        \"description\": \"個人的な悩みや心理的な問題に対してサポートを提供する\",\n",
    "        \"details\": \"個人的な悩みや心理的な問題に対して、共感的で支援的な回答を提供し、可能であれば適切なアドバイスも行ってください。\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb8b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\")\n",
    "llm = ChatOpenAI(model=\"gemma3:12b\", temperature=0.8, openai_api_base=f\"{OLLAMA_HOST}/v1\", openai_api_key=\"dummy\")\n",
    "llm = llm.configurable_fields(max_tokens=ConfigurableField(id=\"max_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d6f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    role_options = \"\\n\".join([f\"{k}. {v['name']}: {v['description']}\" for k, v in ROLES.items()])\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"質問を分析し、最も適切な回答担当ロールを選択してください。\n",
    "\n",
    "選択肢:\n",
    "{role_options}\n",
    "\n",
    "回答は選択肢の番号（1、2、または3）のみを返してください。\n",
    "\n",
    "質問: {query}\n",
    "\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    # 選択肢の番号のみを返すことを期待したいため、max_tokensの値を1に変更\n",
    "    chain = prompt | llm.with_config(configurable=dict(max_tokens=1)) | StrOutputParser()\n",
    "    role_number = chain.invoke({\"role_options\": role_options, \"query\": query}, config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    selected_role = ROLES[(role_number.strip())][\"name\"]\n",
    "\n",
    "    return {\"current_role\": selected_role}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c38d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answering_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    role = state.current_role\n",
    "    role_details = \"\\n\".join([f\"- {v['name']}: {v['details']}\" for v in ROLES.values()])\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"あなたは{role}として回答してください。以下の質問に対して、あなたの役割に基づいた適切な回答を提供してください。\n",
    "\n",
    "役割の詳細:\n",
    "{role_details}\n",
    "\n",
    "質問: {query}\n",
    "\n",
    "回答:\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke(\n",
    "        {\"role\": role, \"role_details\": role_details, \"query\": query}, config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "\n",
    "    return {\"messages\": [answer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816be0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    reason: str = Field(default=\"\", description=\"判定理由\")\n",
    "    judge: bool = Field(default=False, description=\"判定結果\")\n",
    "\n",
    "\n",
    "def check_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    answer = state.messages[-1]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"以下の回答の品質をチェックし、問題がある場合は'False'、問題がない場合は'True'を回答してください。また、その判定理由も説明してください。\n",
    "\n",
    "ユーザーからの質問: {query}\n",
    "回答: {answer}\n",
    "\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm.with_structured_output(Judgement)\n",
    "\n",
    "    result: Judgement = chain.invoke({\"query\": query, \"answer\": answer}, config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    return {\n",
    "        \"current_judge\": result.judge,\n",
    "        \"judgment_reason\": result.reason,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffdbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1db79f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f3b99a06e40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"selection\", selection_node)\n",
    "workflow.add_node(\"answering\", answering_node)\n",
    "workflow.add_node(\"check\", check_node)\n",
    "\n",
    "# selectionノードから処理を開始\n",
    "workflow.set_entry_point(\"selection\")\n",
    "\n",
    "# エッジの接続\n",
    "workflow.add_edge(\"selection\", \"answering\")\n",
    "workflow.add_edge(\"answering\", \"check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62c798d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f3b99a06e40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 条件付きエッジの定義\n",
    "workflow.add_conditional_edges(\"check\", lambda state: state.current_judge, {True: END, False: \"selection\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80523ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7969ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['はい、生成AIについてですね。生成AI製品エキスパートとして、最新の情報と深い洞察を交えながら、分かりやすくご説明します。\\n\\n**1. 生成AIとは何か？**\\n\\n生成AIとは、既存のデータから学習し、新しいコンテンツを生成するAI技術のことです。従来のAIは、データに基づいて予測や分類を行うことが中心でしたが、生成AIはテキスト、画像、音声、動画など、さまざまな形式のコンテンツを「創り出す」ことができます。\\n\\n**例:**\\n\\n*   **文章生成:** 自然な文章を書く (小説、詩、メール、レポートなど)\\n*   **画像生成:** テキスト指示に基づいて画像を生成する (風景、人物、抽象画など)\\n*   **音楽生成:** 既存の音楽スタイルを模倣したり、新しい音楽を作曲する\\n*   **動画生成:** テキストや画像から動画を生成する\\n*   **コード生成:** プログラミングコードを生成する\\n\\n**2. 主なモデルと技術**\\n\\n生成AIを支える主要な技術とモデルをいくつかご紹介します。\\n\\n*   **Transformer:** 現在の多くの生成AIモデルの基盤となる、自然言語処理において優れた性能を発揮する深層学習アーキテクチャです。Attentionメカニズムが特徴で、文章の文脈を理解し、より自然な文章を生成できます。\\n*   **GPT (Generative Pre-trained Transformer):** OpenAIが開発した大規模言語モデルで、GPT-3、GPT-4などが有名です。テキスト生成、翻訳、要約、質疑応答など、幅広いタスクに活用されています。\\n*   **DALL-E:** OpenAIが開発した画像生成AIモデルで、テキストによる指示に基づいてユニークな画像を生成します。\\n*   **Stable Diffusion:** 画像生成AIモデルで、オープンソースであり、商用利用も可能です。\\n*   **Midjourney:** Discord上で動作する画像生成AIで、芸術的な表現に優れています。\\n*   **LoRA (Low-Rank Adaptation):** 大規模言語モデルを特定のタスクやスタイルに微調整する効率的な手法です。\\n\\n**3. 生成AIの活用事例**\\n\\n生成AIは、様々な分野で活用されています。\\n\\n*   **マーケティング:** 広告文の作成、コンテンツ作成\\n*   **クリエイティブ:** イラスト、音楽、動画の制作\\n*   **教育:** 個別指導、教材作成\\n*   **ソフトウェア開発:** コード生成、テスト自動化\\n*   **研究開発:** 新薬開発、材料設計\\n\\n**4. 生成AIの課題と今後の展望**\\n\\n生成AIには、以下のような課題も存在します。\\n\\n*   **倫理的課題:** 著作権侵害、フェイクニュースの拡散、バイアスの増幅など\\n*   **技術的課題:** 幻覚 (事実に基づかない情報を生成する)、制御の難しさ\\n*   **社会経済的課題:** 雇用への影響\\n\\nしかし、これらの課題を克服し、より安全で信頼性の高い生成AIを開発するために、研究開発が活発に進められています。今後は、より高度な生成AIが登場し、私たちの生活やビジネスに大きな影響を与えることが予想されます。\\n\\n**5. 生成AIを始めるには？**\\n\\n*   **無料のオンラインツール:** OpenAI Playground、Google AI Studioなど、無料で試せるツールがあります。\\n*   **APIの利用:** OpenAI API、Stability AI APIなどを利用して、独自のアプリケーションを開発できます。\\n*   **ローカル環境での実行:** モデルをダウンロードして、自分のコンピュータ上で実行することも可能です。\\n\\n**最後に**\\n\\n生成AIは、まだ発展途上の技術ですが、その可能性は計り知れません。ぜひ、色々なツールを試して、その面白さを体験してみてください。\\n\\nもし、特定の分野での活用事例や、より詳細な技術情報について知りたい場合は、お気軽にご質問ください。\\n']\n"
     ]
    }
   ],
   "source": [
    "initial_state = State(query=\"生成AIについて教えてください\")\n",
    "result = compiled.invoke(initial_state, config={\"callbacks\": [langfuse_handler]})\n",
    "print(result[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

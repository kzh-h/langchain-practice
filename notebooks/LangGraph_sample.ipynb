{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5478071c",
   "metadata": {},
   "source": [
    "# LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a98923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "from typing import Annotated, Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebef71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "PUBLIC_KEY = os.getenv(\"PUBLIC_KEY\")\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=PUBLIC_KEY,\n",
    "    secret_key=SECRET_KEY,\n",
    "    host=LANGFUSE_HOST,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb26ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    \"\"\"ステートクラス.\n",
    "\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(..., description=\"ユーザーからの質問\")\n",
    "    current_role: str = Field(default=\"\", description=\"選定された解答ロール\")\n",
    "    # operator.addは2つの値を加算するための関数\n",
    "    # ステート更新時にaddオペレーションにより、リストに要素が追加される。リストの足し算と同義\n",
    "    messages: Annotated[list[str], operator.add] = Field(default=[], description=\"解答履歴\")\n",
    "    current_judge: bool = Field(default=False, description=\"品質チェックの結果\")\n",
    "    judgment_reason: str = Field(default=\"\", description=\"品質チェックの判定理由\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a3c2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLES = {\n",
    "    \"1\": {\n",
    "        \"name\": \"一般知識のエキスパート\",\n",
    "        \"description\": \"幅広い分野の一般的な質問に答える\",\n",
    "        \"details\": \"幅広い分野の一般的な質問に対して、正確でわかりやすい回答を提供してください。\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"name\": \"生成AI製品エキスパート\",\n",
    "        \"description\": \"生成AIや関連製品、技術に関する専門的な質問に答える\",\n",
    "        \"details\": \"生成AIや関連製品、技術に関する専門的な質問に対して、最新の情報と深い洞察を提供してください。\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"name\": \"カウンセラー\",\n",
    "        \"description\": \"個人的な悩みや心理的な問題に対してサポートを提供する\",\n",
    "        \"details\": \"個人的な悩みや心理的な問題に対して、共感的で支援的な回答を提供し、可能であれば適切なアドバイスも行ってください。\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb8b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\")\n",
    "llm = ChatOpenAI(model=\"gemma3:12b\", temperature=0.8, openai_api_base=f\"{OLLAMA_HOST}/v1\", openai_api_key=\"dummy\")\n",
    "llm = llm.configurable_fields(max_tokens=ConfigurableField(id=\"max_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92d6f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    role_options = \"\\n\".join([f\"{k}. {v['name']}: {v['description']}\" for k, v in ROLES.items()])\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"質問を分析し、最も適切な回答担当ロールを選択してください。\n",
    "\n",
    "選択肢:\n",
    "{role_options}\n",
    "\n",
    "回答は選択肢の番号（1、2、または3）のみを返してください。\n",
    "\n",
    "質問: {query}\n",
    "\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    # 選択肢の番号のみを返すことを期待したいため、max_tokensの値を1に変更\n",
    "    chain = prompt | llm.with_config(configurable=dict(max_tokens=1)) | StrOutputParser()\n",
    "    role_number = chain.invoke({\"role_options\": role_options, \"query\": query}, config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    selected_role = ROLES[(role_number.strip())][\"name\"]\n",
    "\n",
    "    return {\"current_role\": selected_role}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c38d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answering_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    role = state.current_role\n",
    "    role_details = \"\\n\".join([f\"- {v['name']}: {v['details']}\" for v in ROLES.values()])\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"あなたは{role}として回答してください。以下の質問に対して、あなたの役割に基づいた適切な回答を提供してください。\n",
    "\n",
    "役割の詳細:\n",
    "{role_details}\n",
    "\n",
    "質問: {query}\n",
    "\n",
    "回答:\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke(\n",
    "        {\"role\": role, \"role_details\": role_details, \"query\": query}, config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "\n",
    "    return {\"messages\": [answer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "816be0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    reason: str = Field(default=\"\", description=\"判定理由\")\n",
    "    judge: bool = Field(default=False, description=\"判定結果\")\n",
    "\n",
    "\n",
    "def check_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    answer = state.messages[-1]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"以下の回答の品質をチェックし、問題がある場合は'False'、問題がない場合は'True'を回答してください。また、その判定理由も説明してください。\n",
    "\n",
    "ユーザーからの質問: {query}\n",
    "回答: {answer}\n",
    "\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm.with_structured_output(Judgement)\n",
    "\n",
    "    result: Judgement = chain.invoke({\"query\": query, \"answer\": answer}, config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "    return {\n",
    "        \"current_judge\": result.judge,\n",
    "        \"judgment_reason\": result.reason,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ffdbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1db79f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f176f675450>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"selection\", selection_node)\n",
    "workflow.add_node(\"answering\", answering_node)\n",
    "workflow.add_node(\"check\", check_node)\n",
    "\n",
    "# selectionノードから処理を開始\n",
    "workflow.set_entry_point(\"selection\")\n",
    "\n",
    "# エッジの接続\n",
    "workflow.add_edge(\"selection\", \"answering\")\n",
    "workflow.add_edge(\"answering\", \"check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d62c798d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f176f675450>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 条件付きエッジの定義\n",
    "workflow.add_conditional_edges(\"check\", lambda state: state.current_judge, {True: END, False: \"selection\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80523ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7969ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['はい、生成AIについてですね。生成AI製品エキスパートとして、わかりやすく解説させていただきます。\\n\\n**1. 生成AIとは何か？**\\n\\n生成AI（Generative AI）とは、既存のデータをもとに、新しいコンテンツを**生成**するAI技術の総称です。従来のAIが「分類」や「予測」といった分析に特化していたのに対し、文章、画像、音楽、コードなど、様々な種類のコンテンツを**創造**できる点が大きな特徴です。\\n\\n*   **例:**\\n    *   文章：小説、詩、メール、ブログ記事などを書く\\n    *   画像：写真、イラスト、絵画などを生成する\\n    *   音楽：作曲、編曲をする\\n    *   コード：プログラミングコードを生成する\\n    *   動画：短い動画クリップを生成する\\n\\n**2. どのようにして動くのか？**\\n\\n生成AIは、主に「深層学習」という技術を用いて、大量のデータからパターンを学習します。代表的なモデルには以下のようなものがあります。\\n\\n*   **Transformer:** 自然言語処理（文章の生成や理解）で非常に高い性能を発揮します。ChatGPTなどの大規模言語モデル(LLM)の基盤技術です。\\n*   **GAN (Generative Adversarial Network):** 敵対的生成ネットワークとも呼ばれ、生成器と識別器という2つのネットワークが互いに競い合いながら学習することで、よりリアルな画像や動画を生成します。\\n*   **Diffusion Models:** 画像生成の分野で近年注目されている手法で、ノイズを徐々に除去していくことで、高画質な画像を生成します。\\n\\n**3. 代表的な生成AI製品とサービス**\\n\\n現在、多くの生成AI製品やサービスが提供されています。\\n\\n*   **ChatGPT:** OpenAIが提供する、文章生成に特化した大規模言語モデル。\\n*   **Bard (Google):** Googleが提供する、ChatGPTと同様の文章生成AI。\\n*   **DALL-E 2 / Midjourney / Stable Diffusion:** OpenAIやStability AIなどが提供する、テキストから画像を生成するAI。\\n*   **GitHub Copilot:** MicrosoftとOpenAIが共同開発した、コーディングを支援するAI。\\n*   **Adobe Firefly:** Adobeが提供する、画像生成や編集を行うAI。\\n\\n**4. 生成AIの可能性と課題**\\n\\n*   **可能性:**\\n    *   コンテンツ制作の効率化：文章作成、デザイン、音楽制作などの時間を大幅に短縮できます。\\n    *   創造性の拡張：新しいアイデアを生み出すためのインスピレーションを与えてくれます。\\n    *   パーソナライズされた体験の提供：個々のユーザーに合わせたコンテンツを生成できます。\\n*   **課題:**\\n    *   倫理的な問題：著作権侵害、フェイクニュースの拡散、バイアスの増幅などのリスクがあります。\\n    *   精度と品質：生成されるコンテンツの質は、学習データに大きく依存します。\\n    *   依存性と創造性の低下：AIに頼りすぎると、人間の創造性が低下する可能性があります。\\n\\n**5. 今後の展望**\\n\\n生成AI技術は急速に進化しており、今後も様々な分野で活用されることが予想されます。より高品質なコンテンツの生成、倫理的な問題への対応、人間の創造性を支援するような使い方が模索されるでしょう。\\n\\n**何か特定の生成AI製品について詳しく知りたい、または、特定の活用事例について知りたいなど、さらに詳しい質問があれば、遠慮なくお尋ねください。** どのような疑問でも、できる限り丁寧にお答えします。']\n"
     ]
    }
   ],
   "source": [
    "initial_state = State(query=\"生成AIについて教えてください\")\n",
    "result = compiled.invoke(initial_state, config={\"callbacks\": [langfuse_handler]})\n",
    "print(result[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
